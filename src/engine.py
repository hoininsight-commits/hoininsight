from __future__ import annotations

import sys
import time
import os
import json
import traceback
from datetime import datetime
from pathlib import Path

from src.reporting.run_log import RunResult, write_run_log, append_observation_log
from src.utils.target_date import get_target_ymd, get_target_parts
from src.pipeline.run_collect import main as collect_main
from src.pipeline.run_normalize import main as normalize_main
from src.pipeline.run_anomaly import main as anomaly_main
from src.pipeline.run_topic import main as topic_main
from src.pipeline.run_topic_gate import main as gate_pipeline_main
from src.reporters.daily_report import write_daily_brief
from src.reporting.health import write_health
from src.validation.output_check import run_output_checks
from src.validation.schema_check import run_schema_checks

def _utc_now_stamp() -> str:
    # Use standardized YMD but append actual UTC time for high-res logs
    return datetime.utcnow().strftime("%Y-%m-%dT%H:%M:%SZ")

def _date_path_standardized() -> Path:
    y, m, d = get_target_parts()
    return Path("data") / "reports" / y / m / d

def main(target_categories: list[str] = None):
    started = _utc_now_stamp()
    start_time = time.time()
    status = "SUCCESS"
    details_lines = []
    check_lines = []
    checks_ok = False
    per_dataset = []

    try:
        details_lines.append("engine: start")
        print("engine: start", file=sys.stderr)
        
        from src.utils.target_date import get_target_ymd
        run_ymd = get_target_ymd()

        collect_main(target_categories)
        details_lines.append("collect: ok")
        print("collect: ok", file=sys.stderr)
        
        normalize_main(target_categories)
        details_lines.append("normalize: ok")
        print("normalize: ok", file=sys.stderr)
        
        # --- Logic Layer ---
        from src.pipeline.derived_metrics_engine import run_derived_metrics
        run_derived_metrics(Path("."))
        details_lines.append("derived: ok")
        print("derived: ok", file=sys.stderr)
        # -------------------
        
        anomaly_main()
        details_lines.append("anomaly: ok")
        print("anomaly: ok", file=sys.stderr)

        # [Step 48] Fact Evidence Harvester (Passive Collection)
        try:
            from src.collectors.fact_evidence_harvester import FactEvidenceHarvester
            feh = FactEvidenceHarvester(Path("."))
            feh.harvest()
            details_lines.append("fact_harvester: ok")
            print("fact_harvester: ok", file=sys.stderr)
        except Exception as e:
            print(f"fact_harvester: error ({e})", file=sys.stderr)

        # [Step 47.5] Fact-First Ingress (Early Topic Capture)
        try:
            from src.ops.fact_first_ingress import FactFirstIngress
            ffi = FactFirstIngress(Path("."))
            # Use current date or detected target if available
            ffi.run_ingress()
            details_lines.append("fact_first_ingress: ok")
            print("fact_first_ingress: ok", file=sys.stderr)
        except Exception as e:
            print(f"fact_first_ingress: error ({e})", file=sys.stderr)

        topic_main()
        details_lines.append("topic: ok")
        print("topic: ok", file=sys.stderr)

        # [NEW] Step 74: Pre-Structural Signal Layer (Economic Hunter Style)
        pre_structural_signals = []
        try:
            from src.ops.pre_structural_signal_layer import PreStructuralSignalLayer
            ps_layer = PreStructuralSignalLayer(Path("."))
            
            # Load candidate topics generated by topic_main()
            y_p, m_p, d_p = get_target_parts()
            topics_dir = Path("data/topics") / y_p / m_p / d_p
            candidate_topics = []
            if topics_dir.exists():
                for f in topics_dir.glob("*.json"):
                    # Skip gate and other subdirs if any, but glob *.json covers dataset files
                    try:
                        t_data = json.loads(f.read_text(encoding="utf-8"))
                        if isinstance(t_data, list):
                            candidate_topics.extend(t_data)
                        elif isinstance(t_data, dict):
                            candidate_topics.append(t_data)
                    except:
                        pass
            
                if candidate_topics:
                    enriched_topics = ps_layer.analyze_topics(candidate_topics)
                    
                    # [MOVED] Step 75: WHY_NOW_ESCALATION_LAYER moved down to follow Step 72
                    
                    pre_structural_signals = [t for t in enriched_topics if t.get("is_pre_structural")]
                
                # Update topic files with signal info
                for et in enriched_topics:
                    ds_id = et.get("dataset_id")
                    if ds_id:
                        t_path = topics_dir / f"{ds_id}.json"
                        t_path.write_text(json.dumps([et], indent=2, ensure_ascii=False), encoding="utf-8")
            
            # Generate Step 74 Report
            report_dir = Path("data/reports") / y_p / m_p / d_p
            report_dir.mkdir(parents=True, exist_ok=True)
            report_path = report_dir / "report_step74_pre_structural_signal.md"
            
            report_lines = [
                "# report_step74_pre_structural_signal.md",
                f"Date: {run_ymd}",
                "\n## 1. Definition Recap",
                "Pre-Structural Signal: Market-moving event where narrative/expectation begins reallocating capital BEFORE legal/policy/earnings confirmation.",
                "\n## 2. Detected Signals",
            ]
            
            if not pre_structural_signals:
                report_lines.append("- (No pre-structural signals detected today)")
            else:
                for sig in pre_structural_signals:
                    ctx = sig["pre_structural_signal"]
                    report_lines.append(f"### Topic: {sig.get('title')}")
                    report_lines.append(f"- **Signal Type:** {ctx.get('signal_type')}")
                    report_lines.append(f"- **Trigger Actor:** {ctx.get('trigger_actor')}")
                    report_lines.append(f"- **Temporal Anchor:** {ctx.get('temporal_anchor')}")
                    report_lines.append(f"- **Unresolved Question:** {ctx.get('unresolved_question')}")
                    report_lines.append(f"- **Rationale:** {ctx.get('rationale')}")
                    report_lines.append(f"- **Upgrade Condition:** {ctx.get('escalation_path', {}).get('condition_to_upgrade_to_WHY_NOW')}")
                    report_lines.append(f"- **Invalidation Condition:** {ctx.get('escalation_path', {}).get('condition_to_invalidate')}")
                    report_lines.append("")
            
            report_path.write_text("\n".join(report_lines), encoding="utf-8")
            details_lines.append("pre_structural_layer: ok")
            print("pre_structural_layer: ok", file=sys.stderr)
            
        except Exception as e:
            print(f"pre_structural_layer: fail ({e})", file=sys.stderr)
            traceback.print_exc()

        # [Phase 50] Anchor Engine (Economy Hunter Logic) - Strict 6-Step Enforcement
        from src.topics.anchor_engine.logic_core import AnchorEngine
        
        # Load Anomalies for Anchor Input
        anomalies_dir = Path("data/features/anomalies") / datetime.now().strftime("%Y/%m/%d")
        snapshots = []
        if anomalies_dir.exists():
            for f in anomalies_dir.glob("*.json"):
                try: 
                    payload = json.loads(f.read_text(encoding="utf-8"))
                    dataset_id = payload.get("dataset_id", f.stem) if isinstance(payload, dict) else f.stem
                    
                    if isinstance(payload, list):
                        for item in payload:
                            if isinstance(item, dict) and "dataset_id" not in item:
                                item["dataset_id"] = dataset_id
                        snapshots.extend(payload)
                    elif isinstance(payload, dict):
                        if "anomalies" in payload and isinstance(payload["anomalies"], list):
                             for item in payload["anomalies"]:
                                 if isinstance(item, dict) and "dataset_id" not in item:
                                     item["dataset_id"] = dataset_id
                             snapshots.extend(payload["anomalies"])
                        else:
                             if "dataset_id" not in payload:
                                 payload["dataset_id"] = dataset_id
                             snapshots.append(payload)
                except Exception as e:
                    print(f"Error loading {f}: {e}", file=sys.stderr)

        anchor_engine = AnchorEngine(Path("."))
        anchor_results = anchor_engine.run_analysis(snapshots, pre_structural_signals)
        
        # Save Anchor Result
        anchor_out_dir = Path("data/topics/anchor") / datetime.now().strftime("%Y/%m/%d")
        anchor_out_dir.mkdir(parents=True, exist_ok=True)
        
        # Pick Best Anchor (First one for now, or sort by Level)
        # Sorting: L4 > L3 > L2
        anchor_results.sort(key=lambda x: {"L4": 3, "L3": 2, "L2": 1}.get(x.level, 0), reverse=True)
        
        if anchor_results:
            best_anchor = anchor_results[0]
            anchor_file = anchor_out_dir / "anchor_result.json"
            # Serialize
            import dataclasses
            with open(anchor_file, "w", encoding="utf-8") as f:
                json.dump(dataclasses.asdict(best_anchor), f, indent=2, ensure_ascii=False)
            print(f"[Anchor] Selected Best Logic: {best_anchor.anomaly_logic} ({best_anchor.level})", file=sys.stdout)
        else:
            print("[Anchor] No valid topic found matching Anchor Logic steps.", file=sys.stdout)

        details_lines.append("anchor_engine: ok")
        print("anchor_engine: ok", file=sys.stderr)

        # [Phase 50] Strict Topic Decision Gate (Engine 2)
        # 1. Ensure Data Snapshot (JSON) exists
        from src.reporters.data_snapshot import write_data_snapshot
        run_ymd = get_target_ymd()
        try:
            snapshot_path = write_data_snapshot(Path("."))
            details_lines.append(f"snapshot: ok ({snapshot_path})")
            print("snapshot: ok", file=sys.stderr)
        except Exception as e:
             print(f"snapshot: warn ({e})", file=sys.stderr)

        # [NEW] Step 41: Run Pick-Outcome Correlator (Accountability)
        from src.ops.pick_outcome_correlator import PickOutcomeCorrelator
        try:
            correlator = PickOutcomeCorrelator(Path("."))
            correlator.run(run_ymd)
            details_lines.append("pick_correlator: ok")
            print("pick_correlator: ok", file=sys.stderr)
        except Exception as e:
            print(f"pick_correlator: fail ({e})", file=sys.stderr)

        # 2. Run Gate Pipeline (Content Hook Logic)
        try:
            gate_pipeline_main(run_ymd)
            details_lines.append("topic_gate: ok")
            print("topic_gate: ok", file=sys.stderr)
        except Exception as e:
            print(f"topic_gate: fail ({e})", file=sys.stderr)

        # [NEW] Step 43 & 44: Auto Prioritization & Approval
        # These will be executed inside DecisionDashboard.build_dashboard_data
        # to ensure they have access to the fully ranked topics and shadow pool.
        
        # [Step 52] Topic View Panel (Read-only consolidation)
        try:
            from src.ops.topic_view_builder import TopicViewBuilder
            tvb = TopicViewBuilder(Path("."))
            topic_view_today = tvb.build_view(run_ymd)
            details_lines.append("topic_view: ok")
            print("topic_view: ok", file=sys.stderr)
            
            # [NEW] Step 57: Human Preference Overlay
            try:
                from src.ops.human_preference_overlay import HumanPreferenceOverlay
                hpo = HumanPreferenceOverlay(Path("."))
                hpo.build_signature()
                hpo.evaluate_today(topic_view_today)
                details_lines.append("human_pref_overlay: ok")
                print("human_pref_overlay: ok", file=sys.stderr)
            except Exception as e_hpo:
                print(f"human_pref_overlay: error ({e_hpo})", file=sys.stderr)

            # [NEW] Step 58: Calibration Explainability Report
            try:
                from src.ops.calibration_explainability_report import CalibrationExplainabilityReport
                cer = CalibrationExplainabilityReport(Path("."))
                cer.run(run_ymd)
                details_lines.append("calibration_report: ok")
                print("calibration_report: ok", file=sys.stderr)
            except Exception as e_cer:
                print(f"calibration_report: error ({e_cer})", file=sys.stderr)

        except Exception as e:
            print(f"topic_view: error ({e})", file=sys.stderr)

        # [NEW] Generate Decision Dashboard Markdown
        from src.reporters.decision_dashboard import DecisionDashboard
        try:
            dd = DecisionDashboard(Path("."))
            # Step 43/44 are now deeply integrated into build_dashboard_data
            dd_data = dd.build_dashboard_data(run_ymd)
            
            # [NEW] Step 53: Topic Quality Review Execution
            try:
                from src.ops.topic_quality_review import TopicQualityReview
                tqr = TopicQualityReview(Path("."))
                qr_res = tqr.run(run_ymd, dd_data.get("cards", []))
                dd_data["quality_review"] = qr_res
                details_lines.append("topic_quality_review: ok")
                print("topic_quality_review: ok", file=sys.stderr)
            except Exception as e:
                print(f"topic_quality_review: run error ({e})", file=sys.stderr)

            # [NEW] Step 54: Editorial Speakability Gate Execution
            try:
                from src.ops.editorial_speakability_gate import EditorialSpeakabilityGate
                esg = EditorialSpeakabilityGate(Path("."))
                es_res = esg.run(run_ymd)
                dd_data["speakability"] = es_res
                details_lines.append("speakability_gate: ok")
                print("speakability_gate: ok", file=sys.stderr)
            except Exception as e:
                print(f"speakability_gate: run error ({e})", file=sys.stderr)
            
            # Write auto_approved_today.json specifically if not already saved by dd
            # Actually, let's make sure AutoApprovalGate runs and saves.
            from src.ops.auto_approval_gate import AutoApprovalGate
            from src.ops.operator_decision_log import OperatorDecisionLog
            
            # We need the cards from dd_data
            ready_topics = [c for c in dd_data.get("cards", []) if c["status"] == "READY"]
            ap_data = dd_data.get("auto_priority", {})
            op_log = OperatorDecisionLog(Path("."))
            op_decisions = op_log.get_latest_decisions_map(run_ymd)
            
            aa_gate = AutoApprovalGate(Path("."))
            aa_data = aa_gate.run(run_ymd, ready_topics, ap_data, op_decisions)
            
            # [NEW] Step 45: Run Speak Bundle Exporter
            from src.ops.speak_bundle_exporter import SpeakBundleExporter
            try:
                exporter = SpeakBundleExporter(Path("."))
                export_res = exporter.run(run_ymd, dd_data.get("cards", []), aa_data)
                details_lines.append("speak_bundle: ok")
                print("speak_bundle: ok", file=sys.stderr)
                
                # [NEW] Step 47: Run Script Skeleton Generator
                from src.ops.script_skeleton_generator import ScriptSkeletonGenerator
                try:
                    # Load the generated bundle json to get the accurate topics list
                    bundle_json_path = Path(".") / export_res["json_path"]
                    if bundle_json_path.exists():
                        bundle_content = json.loads(bundle_json_path.read_text(encoding="utf-8"))
                        sk_gen = ScriptSkeletonGenerator(Path("."))
                        sk_gen.run(run_ymd, bundle_content)
                        details_lines.append("script_skeletons: ok")
                        print("script_skeletons: ok", file=sys.stderr)
                except Exception as e_sk:
                    print(f"script_skeletons: fail ({e_sk})", file=sys.stderr)
                    
            except Exception as e:
                print(f"speak_bundle: fail ({e})", file=sys.stderr)

            # [NEW] Step 55: Topic Console Builder Execution
            try:
                from src.ops.topic_console_builder import TopicConsoleBuilder
                tcb = TopicConsoleBuilder(Path("."))
                tc_res = tcb.run(run_ymd)
                dd_data["topic_console"] = tc_res
                details_lines.append("topic_console: ok")
                print("topic_console: ok", file=sys.stderr)
            except Exception as e:
                print(f"topic_console: run error ({e})", file=sys.stderr)

            # [NEW] Step 63: Structural Topic Seeds Execution
            try:
                from src.ops.structural_seeder import StructuralTopicSeeder
                seeder = StructuralTopicSeeder(Path("."))
                seeder.run()
                details_lines.append("structural_seeder: ok")
                print("structural_seeder: ok", file=sys.stderr)
            except Exception as e:
                print(f"structural_seeder: fail ({e})", file=sys.stderr)

            # [NEW] Step 64: IssueSignal Builder Execution
            try:
                from src.ops.issue_signal_builder import IssueSignalBuilder
                builder = IssueSignalBuilder(Path("."))
                builder.run()
                details_lines.append("issuesignal_builder: ok")
                print("issuesignal_builder: ok", file=sys.stderr)
            except Exception as e:
                print(f"issuesignal_builder: fail ({e})", file=sys.stderr)

            # [NEW] Step 65: Video Candidate Selection Execution
            try:
                from src.ops.video_candidate_selector import VideoCandidateSelector
                selector = VideoCandidateSelector(Path("."))
                selector.run()
                details_lines.append("video_selector: ok")
                print("video_selector: ok", file=sys.stderr)
            except Exception as e:
                print(f"video_selector: fail ({e})", file=sys.stderr)

            # [NEW] Step 66: Structural Top-1 Compressor Execution
            try:
                from src.ops.structural_top1_compressor import StructuralTop1Compressor
                compressor = StructuralTop1Compressor(Path("."))
                compressor.run()
                details_lines.append("top1_compressor: ok")
                print("top1_compressor: ok", file=sys.stderr)
            except Exception as e:
                print(f"top1_compressor: fail ({e})", file=sys.stderr)

            # [REORDERED] Step 72: WHY_NOW Trigger Layer Execution (Additive)
            # Must run before Lock Layer to provide whynow_trigger
            try:
                from src.ops.whynow_trigger_layer import WhyNowTriggerLayer
                whynow = WhyNowTriggerLayer(Path("."))
                whynow.run()
                details_lines.append("whynow_trigger: ok")
                print("whynow_trigger: ok", file=sys.stderr)
            except Exception as e:
                 print(f"whynow_trigger: fail ({e})", file=sys.stderr)

            # [REORDERED] Step 75: WHY_NOW_ESCALATION_LAYER Execution
            # Now runs after 72 and before 76 as per Step 77 requirement 6
            try:
                # We need to reload the top1 data after whynow_trigger might have changed it,
                # but whynow_trigger_layer.py usually writes to issue_signal_narrative_today.json.
                # whynow_escalation_layer.py evaluate_signals expects a list of topics.
                # For engine reordering, we can run it on the current Top-1.
                from src.ops.whynow_escalation_layer import WhyNowEscalationLayer
                esc_layer = WhyNowEscalationLayer(Path("."))
                
                # Load current Top-1 to escalate
                top1_path = Path("data/ops/structural_top1_today.json")
                if top1_path.exists():
                    t1_data = json.loads(top1_path.read_text(encoding='utf-8'))
                    if t1_data.get("top1_topics"):
                        t1_data["top1_topics"] = esc_layer.evaluate_signals(t1_data["top1_topics"])
                        top1_path.write_text(json.dumps(t1_data, indent=2, ensure_ascii=False), encoding='utf-8')
                        details_lines.append("whynow_escalation: ok")
                        print("whynow_escalation: ok", file=sys.stderr)
            except Exception as e:
                print(f"whynow_escalation: fail ({e})", file=sys.stderr)

            # [NEW] Step 76: Economic Hunter Topic Lock Layer
            topic_lock = False
            try:
                from src.ops.economic_hunter_topic_lock_layer import EconomicHunterTopicLockLayer
                lock_layer = EconomicHunterTopicLockLayer(Path("."))
                lock_result = lock_layer.evaluate_lock()
                topic_lock = lock_result.get("topic_lock", False)
                details_lines.append(f"topic_lock: ok (lock={topic_lock})")
                print(f"topic_lock: ok (lock={topic_lock})", file=sys.stderr)
            except Exception as e:
                print(f"topic_lock: fail ({e})", file=sys.stderr)

            # [NEW] Step 77: Economic Hunter Video Intensity Decision
            if topic_lock:
                try:
                    from src.ops.economic_hunter_video_intensity import EconomicHunterVideoIntensityLayer
                    intensity_layer = EconomicHunterVideoIntensityLayer(Path("."))
                    intensity_res = intensity_layer.decide_intensity()
                    details_lines.append(f"video_intensity: ok ({intensity_res.get('level', 'REJECTED')})")
                    print(f"video_intensity: ok ({intensity_res.get('level', 'REJECTED')})", file=sys.stderr)
                    # If intensity decision triggered a reject, disable lock
                    if intensity_res.get("status") == "rejected":
                        topic_lock = False
                except Exception as e:
                    print(f"video_intensity: fail ({e})", file=sys.stderr)

            # [NEW] Step 78: Economic Hunter Video Rhythm Decision
            if topic_lock:
                try:
                    from src.ops.economic_hunter_video_rhythm import EconomicHunterVideoRhythmLayer
                    rhythm_layer = EconomicHunterVideoRhythmLayer(Path("."))
                    rhythm_res = rhythm_layer.assign_rhythm()
                    details_lines.append(f"video_rhythm: ok ({rhythm_res.get('rhythm_profile', 'REJECTED')})")
                    print(f"video_rhythm: ok ({rhythm_res.get('rhythm_profile', 'REJECTED')})", file=sys.stderr)
                    if rhythm_res.get("status") == "rejected":
                         topic_lock = False
                except Exception as e:
                    print(f"video_rhythm: fail ({e})", file=sys.stderr)

            # [NEW] Step 67: Narrator Selection & Execution
            if topic_lock:
                try:
                    from src.ops.economic_hunter_narrator import EconomicHunterNarrator
                    narrator = EconomicHunterNarrator(Path("."))
                    narrator.run()
                    details_lines.append("economic_hunter_narrator: ok (LOCKED)")
                    print("economic_hunter_narrator: ok (LOCKED)", file=sys.stderr)
                except Exception as e:
                    print(f"economic_hunter_narrator: fail ({e})", file=sys.stderr)
            else:
                # Fallback to standard report or existing narrator
                print("topic_lock: false -> Using Standard Report Narrator", file=sys.stderr)
                # (Existing logic for standard report would go here or fallback to Step 67)
                try:
                    # For now, we still use EconomicHunterNarrator but it will handle non-locked topics if needed,
                    # OR we implement a specific ReportNarrator. 
                    # Requirement says: "topic_lock == false 인 경우: 기존 Report Narrator 사용"
                    # If ReportNarrator is not yet implemented, we might need a placeholder.
                    # Let's check for ReportNarrator or similar.
                    from src.ops.economic_hunter_narrator import EconomicHunterNarrator
                    narrator = EconomicHunterNarrator(Path("."))
                    narrator.run()
                    details_lines.append("economic_hunter_narrator: ok (Standard)")
                except Exception as e:
                    print(f"narrator_fallback: fail ({e})", file=sys.stderr)

            dd_md = dd.render_markdown(dd_data)
            
            y, m, d = get_target_parts()
            dd_path = Path("data/reports") / y / m / d / "decision_dashboard.md"
            dd_path.parent.mkdir(parents=True, exist_ok=True)
            dd_path.write_text(dd_md, encoding="utf-8")
            
            details_lines.append(f"decision_dashboard: ok | {dd_path.as_posix()}")
            print(f"decision_dashboard: ok", file=sys.stderr)
        except Exception as e:
            print(f"decision_dashboard: fail ({e})", file=sys.stderr)
            traceback.print_exc(file=sys.stderr)

        # [Fixed] Ensure Final Decision Card is generated
        from src.decision.final_decision_card import main as decision_main
        decision_main()
        details_lines.append("decision: ok")
        print("decision: ok", file=sys.stderr)

        report_path = write_daily_brief(Path("."))
        details_lines.append(f"report: ok | {report_path.as_posix()}")
        print(f"report: ok | {report_path.as_posix()}", file=sys.stderr)

        chk = run_output_checks(Path("."), target_categories)
        check_lines = chk.lines
        per_dataset = chk.per_dataset
        checks_ok = chk.ok
        details_lines.extend(["checks:"] + chk.lines)
        if not chk.ok:
            # We print detailed validation errors to stderr
            for line in chk.lines:
                print(f"validation: {line}", file=sys.stderr)
            raise RuntimeError("output checks failed")

        sch = run_schema_checks(Path("."), target_categories)
        details_lines.extend(["schema_checks:"] + sch.lines)
        if not sch.ok:
            for line in sch.lines:
                print(f"schema: {line}", file=sys.stderr)
            raise RuntimeError("schema checks failed")

        details_lines.append("engine: done")
        print("engine: done", file=sys.stderr)
        
    except Exception as e:
        status = "FAIL"
        err_msg = f"error: {repr(e)}"
        details_lines.append(err_msg)
        # CRITICAL: Print error to stderr so it appears in CI logs
        print(err_msg, file=sys.stderr)
        traceback.print_exc(file=sys.stderr)

    health_path = write_health(Path("."), status=status, checks_ok=checks_ok, check_lines=check_lines, per_dataset=per_dataset)
    details_lines.append(f"health: {health_path.as_posix()}")

    # --- Batch Timing Verification (Ops) ---
    try:
        runtime_path = Path("data/ops/workflow_runtime_v1.json")
        if runtime_path.exists():
            ops_data = json.loads(runtime_path.read_text(encoding="utf-8"))
            # Detect workflow from env or assume standard based on category
            current_workflow = os.environ.get("GITHUB_WORKFLOW", "unknown_workflow")
            
            # Check if we can map categories to known workflows for local testing
            if current_workflow == "unknown_workflow":
                if target_categories and "CRYPTO" in target_categories: current_workflow = "pipeline_crypto.yml"
                elif target_categories and "FX_RATES" in target_categories: current_workflow = "pipeline_fx.yml"
                elif target_categories and "US_MARKETS" in target_categories: current_workflow = "pipeline_us_markets.yml"
                elif target_categories and "BACKFILL" in target_categories: current_workflow = "pipeline_backfill.yml"
            
            target_p95 = 300 # Default fallback
            for wf in ops_data["workflows"]:
                if wf["workflow"] == current_workflow or wf["workflow"] in current_workflow:
                    target_p95 = wf["p95_duration_sec"]
                    break
            
            actual_duration = round(time.time() - start_time, 2)
            if actual_duration <= target_p95:
                print(f"[VERIFY][OK] Workflow completed within expected duration ({target_p95}s)", file=sys.stdout)
            else:
                print(f"[VERIFY][WARN] Workflow exceeded expected duration (actual {actual_duration}s > p95 {target_p95}s)", file=sys.stdout)
    except Exception as e:
        print(f"[VERIFY][SKIP] Could not verify runtime: {e}", file=sys.stderr)
    # ----------------------------------------

    finished = _utc_now_stamp()
    report_dir = _date_path_standardized()
    result = RunResult(
        started_utc=started,
        finished_utc=finished,
        status=status,
        details="\n".join(details_lines),
    )
    log_path = write_run_log(report_dir, result)

    obs_line = f"- {finished} | engine_run | status={status} | run_log={log_path.as_posix()}\n"
    append_observation_log(Path("docs") / "OBSERVATION_LOG.md", obs_line)

    # [Step 54] Final: Update Dashboard Manifest
    try:
        from src.ops.dashboard_manifest import DashboardManifest
        manifest = DashboardManifest(Path("."))
        manifest.generate(run_ymd)
        print(f"Manifest updated for {run_ymd}", file=sys.stderr)
    except Exception as e:
        print(f"Manifest update error: {e}", file=sys.stderr)

    if status != "SUCCESS":
        sys.exit(1)

if __name__ == "__main__":
    main()
