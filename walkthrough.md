# Walkthrough - RUN_DATE Cohesion and Dashboard Standardization

We have standardized the date handling across the HOIN ENGINE to ensure all artifacts produced in a single run share the same `RUN_YMD` (KST, UTC+9). This ensures the dashboard correctly links to the latest decision data without stale or mismatched paths.

## Changes Made

### 1. RUN_DATE/RUN_YMD Standardization
- **[target\_date.py](file:///Users/jihopa/.gemini/antigravity/scratch/HoinInsight/src/utils/target_date.py)**: Updated to use **KST (UTC+9)** as the default for `get_target_ymd()`.
- **[engine.py](file:///Users/jihopa/.gemini/antigravity/scratch/HoinInsight/src/engine.py)**: Revised to use standardized `RUN_YMD` for folder creation and artifact generation.

### 2. Artifact and Dashboard Alignment
- **[decision\_dashboard.md](file:///Users/jihopa/.gemini/antigravity/scratch/HoinInsight/data/reports/2026/01/26/decision_dashboard.md)**: Now explicitly generated by the engine and saved in the daily report folder.
- **[latest\_run.json](file:///Users/jihopa/.gemini/antigravity/scratch/HoinInsight/data/dashboard/latest_run.json)**: Updated manifest to point `decision_md` to the new `decision_dashboard.md`.
- **[health\_today.json](file:///Users/jihopa/.gemini/antigravity/scratch/HoinInsight/data/dashboard/health_today.json)**: Implementation in `health_check.py` now scans real artifact paths and reports errors if counts are zero or files are missing.

### 3. Cleanup and Verification
- Standardized date handling in `daily_report.py`, `data_snapshot.py`, and `health.py`.
- Verified Dashboard UI cohesion with the new manifest structure.

## How to interpret events_count == 0

When `events_count` is 0, the system now provides an **Input Status** code in the dashboard:

- **OUTPUT_EMPTY_VALID**: No events were detected today, but the pipeline executed correctly. This is a "real no-topic day".
- **FETCH_FAILED**: The registry has enabled sources, but no output was generated. This suggests a network issue or a pipeline crash.
- **SOURCE_REGISTRY_MISSING/EMPTY**: The configuration that defines where to get events is missing or has no active sources.
- **PARSE_FAILED**: An error occurred while reading the `events.json` file.
- **OUTPUT_PATH_MISSING**: The file doesn't exist and the registry check was inconclusive.

## Step 32: Live Run Verification & Operator Closure

We verified the end-to-end pipeline in GitHub Actions (Run ID: 21345027092).

### Verification Evidence
- **RUN_YMD**: 2026-01-26
- **Events Count**: 0
- **Input Status**: `FETCH_FAILED` (Correctly diagnosed due to active registry but missing output)
- **Operator Closure**: The dashboard correctly displays: `ğŸš¨ ì˜¤ëŠ˜ì€ ì´ë²¤íŠ¸ ì…ë ¥ì´ 0ê±´ì´ë¼ í† í”½ ìƒì„±ì´ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤. (ROOT: FETCH_FAILED)`

## Step 33: Event Coverage & Last-Seen Monitor

We implemented an event coverage monitoring system to make "event input missing" fully diagnosable.

### Key Deliverables
- **Coverage Builder**: `src/ops/event_coverage.py` scans artifacts and computes source freshness.
- **Pipeline Integration**: Automatically runs after health check in `full_pipeline.yml`.
- **Dashboard Panel**: New "EVENT COVERAGE" tab with dynamic loading and operator hints.

### Verification Evidence
- **Global Flags**: `NO_EVENTS_ALL` correctly triggered when today's events are zero.
- **Last Seen**: Correctly identifies `2026-01-24` as the last valid date for all registered sources.
- **Operator Hint**: Dashboard displays: `ğŸ’¡ ì…ë ¥ ì´ë²¤íŠ¸ 0ê±´ â€” ì†ŒìŠ¤ ìŠ¤í†¨ / ê²½ë¡œ / ìŠ¤ì¼€ì¤„ ë¬¸ì œ ê°€ëŠ¥ì„±`

## Step 34: Shadow Candidate Pool (Continuity Layer)

We implemented a "Shadow Candidate Pool" to provide continuity and preparation for topics that are structurally promising but not yet speakable.

### Key Deliverables
- **Shadow Builder**: `src/ops/shadow_candidate_builder.py` identifies eligible topics (HOLD/DROP, Fact-driven or Structural, Non-immediate).
- **Preparation Pool**: Topics are stored with explicit "Promotion Triggers" (e.g., additional evidence or signal confirmation).
- **Conditional Rendering**: The dashboard renders this section only when `READY < 3`, ensuring the operator always has a look-ahead even when today's output is low.

### Verification Evidence
- **Eligibility**: Topics rejected for critical reasons (TITLE_MISMATCH) or purely placeholder evidence are correctly excluded.
- **Visuals**: Section uses grey tones (â—½ ğŸ”˜) and explicit "NOT FOR NARRATION YET" labels.
- **Tests**: `tests/test_shadow_candidate_builder.py` passes 3/3.

## Step 35: Shadow â†’ Trigger Map (Promotion Readiness)

We enhanced the Shadow Candidate Pool by mapping each candidate to specific actionable triggers, transforming the pool into a promotion-readiness map.

### Key Deliverables
- **Trigger Map Builder**: `src/ops/trigger_map.py` defines a `TRIGGER_ENUM` (e.g., `NUMERIC_EVIDENCE_APPEAR`, `SUPPORTING_ANOMALY_DETECTED`) and maps failure codes to these triggers.
- **Actionable Hints**: Each shadow candidate now lists what logic is missing, where the evidence is expected from (e.g., earnings release), and when it will be re-checked (e.g., `WINDOW_OPEN`).
- **Dashboard Extended**: The "SHADOW CANDIDATES" section now includes a "PROMOTION READINESS" block for each candidate, aiding operator look-ahead.

### Verification Evidence
- **Deterministic Mapping**: Failure codes like `LOW_EVIDENCE` consistently map to `NUMERIC_EVIDENCE_APPEAR`.
- **UI Consistency**: Grey-toned rendering maintained with explicit "Still NOT for narration" labels.
- **Tests**: `tests/test_trigger_map.py` passes 4/4.

## Step 36: Shadow Aging & Decay Guard

We introduced an aging and decay monitoring system for shadow candidates to prevent "silent rotting" and maintain operational hygiene.

### Key Deliverables
- **Aging Calculator**: `src/ops/shadow_aging.py` classifies candidates into `FRESH`, `STALE`, `DECAYING`, or `EXPIRED` based on first-seen duration.
- **Lookback Integration**: `ShadowCandidateBuilder` now scans 60 days of history to determine the `first_seen_date` for each shadow topic.
- **Aging Summary Panel**: A high-level overview of the shadow pool's "health" (FRESH vs. EXPIRED) displayed at the top of the shadow section.
- **Enhanced Cards**: Shadow cards now explicitly display their age and a governance hint (e.g., âš ï¸ `DECAYING`, ğŸ§Š `EXPIRED`).

### Verification Evidence
- **Deterministic Buckets**: Verified correct classification boundaries (7d, 21d, 45d) via `tests/test_shadow_aging.py`.
- **Governance Hints**: Visual rules properly applied (warnings for stale topics, fading for expired ones).
- **Tests**: `tests/test_shadow_aging.py` passed.

## Step 37: Signal Watchlist (Input Monitoring)

We transformed the Shadow Pool into an explicit Signal Watchlist, making it clear to the operator what specific data triggers the engine is waiting for.

### Key Deliverables
- **Watchlist Builder**: `src/ops/signal_watchlist.py` uses a fixed `WATCH_SIGNAL_ENUM` (e.g., `EARNINGS_RELEASE`, `ONCHAIN_CONFIRMATION`) to monitor shadow candidates.
- **Global Monitoring Panel**: A new dashboard section, `GLOBAL SIGNAL WATCHLIST`, aggregates all pending signals across the shadow pool, providing high-level operational awareness.
- **Signal-Specific Blocks**: Each shadow candidate card now includes a `SIGNAL WATCHLIST` block that details exactly which signals are missing and when the next recheck is scheduled.

### Verification Evidence
- **Enum Determinism**: All watch signals are mapped from a strict enum based on deterministic rules and source hints.
- **Aging Preservation**: Shadow aging and decay status are correctly maintained alongside the new watchlist metadata.
- **Tests**: `tests/test_signal_watchlist.py` passed with 4/4 assertions.

## Step 38: Signal Arrival â†” Shadow Matching

We connected the shadow monitoring watchlist with the actual pipeline outputs, allowing the system to automatically detect when a "waiting-for" signal has actually arrived.

### Key Deliverables
- **Signal Arrival Matcher**: `src/ops/signal_arrival_matcher.py` scans today's artifacts (e.g., `events.json`, `daily_snapshot.json`) to detect signal arrivals (e.g., `EARNINGS_RELEASE`, `NUMERIC_EVIDENCE_APPEAR`).
- **Matching Intelligence**: The shadow builder now automatically compares watchlists against arrived signals, attaching clear match status to each candidate.
- **Signal Status UI**: Shadow cards now feature a `SIGNAL STATUS` block with icons (âœ… Arrived / â³ Still waiting).
- **Global Arrival Panel**: A new `SIGNAL ARRIVAL TODAY` panel provides a high-level summary of which monitored signals were satisfied today.

### Verification Evidence
- **Source Integrity**: Arrivals are triggered only by explicit artifact presence or content (e.g., searching for "earnings" in events).
- **Matching Determinism**: Verified that shadow candidates correctly reflect their specific match status via `tests/test_signal_arrival_matcher.py`.
- **No Promotion**: Confirmed that signal arrival does NOT auto-promote topics to READY, maintaining human-in-the-loop control.
- **Tests**: 3/3 tests passed.

## Step 39: Promotion Readiness Scoreboard

We transformed the Shadow Pool into a deterministic "Promotion Readiness Scoreboard," allowing operators to see at a glance how close each promising topic is to being narratable.

### Key Deliverables
- **Readiness Calculator**: `src/ops/promotion_readiness.py` maps candidates to readiness buckets (`READY_TO_PROMOTE`, `NEARLY_READY`, etc.) based on the signal gap.
- **Progress Visibility**: Each shadow card now displays its specific progress (e.g., `2 / 3 signals`) and explicitly lists what is still missing.
- **Global Scoreboard**: A new dashboard summary panel aggregates the entire pool by readiness state, highlighting immediately actionable opportunities.
- **Operator Hints**: Added deterministic action hints (e.g., "Watch for {signal}" or "Re-run pipeline") to guide human decision-making without auto-promotion.

### Verification Evidence
- **Deterministic Buckets**: Verified bucket transitions (0 missing â†’ READY, 1 missing â†’ NEARLY, etc.) via `tests/test_promotion_readiness.py`.
- **UI Affirmation**: `READY_TO_PROMOTE` candidates are clearly highlighted with a âœ… icon to signal promotion potential to the operator.
- **Safety**: Confirmed that no automatic state changes occur in the core Gate/Bridge/Ranker logic.
- **Tests**: 3/3 tests passed.

## Step 40: Operator Decision Loop Closure

We implemented an append-only editorial logging system to capture human actions (Pick, Skip, Defer, Reject) directly into the operational pipeline, closing the feedback loop between the engine's suggestions and the operator's final choices.

### Key Deliverables
- **Decision Log**: `src/ops/operator_decision_log.py` provides a daily append-only persistent record of human editorial choices in `data/ops/operator_decisions/YYYY-MM-DD.json`.
- **Editorial Summary**: A new dashboard panel, `OPERATOR DECISIONS (TODAY)`, provides high-level accountability for daily editorial throughput.
- **Action Capture UI**: Dashboard candidate cards (READY and SHADOW) now feature explicit action bars and status badges (ğŸ§­ OPERATOR: ...) to reflect captured decisions.
- **Auditability**: Every decision is logged with a UTC timestamp and an optional context note, ensuring full traceability for post-mortem reviews.

### Verification Evidence
- **Append-Only Integrity**: Verified that multiple decisions for the same topic are appended correctly, with the latest choice reflected on the dashboard.
- **State Protection**: Confirmed that logging human decisions never alters the underlying engine state (READY/HOLD/DROP) or topic rankings.
- **UI Affirmation**: Successfully verified badge rendering for recorded decisions in the Decision Dashboard.
- **Tests**: 3/3 tests passed in `tests/test_operator_decision_log.py`.

## Step 41: Pick â†” Outcome Correlation View

We introduced a "Pick â†’ Outcome" correlation system to provide transparency and accountability for human editorial decisions. This view connects operator picks with the eventual engine-determined outcomes (CONFIRMED/FAILED).

### Key Deliverables
- **Correlation Engine**: `src/ops/pick_outcome_correlator.py` performs a strict join between operator logs and retrospective post-mortem data over a rolling 30-day window.
- **Scoreboard Dashboard**: A new dashboard panel, `ğŸ“Œ PICK â†’ OUTCOME (Last 30d)`, summarizes collective human picking performance and lists recent results.
- **Performance Summary**: Automatically aggregates counts for Picked, Confirmed, Failed, Unresolved, and Missing outcomes.
- **Join Integrity**: Implented strict `topic_id` matching with title-based fallback to ensure data accuracy across disparate lookback logs.

### Verification Evidence
- **Strict Join Logic**: Verified ID-first matching and normalized title fallback via `tests/test_pick_outcome_correlator.py`.
- **Summary Accuracy**: Confirmed that outcome counts correctly reflect the underlying history records.
- **UI Rendering**: Dashboard successfully renders the correlation panel even when historical records for a specific day are unresolved or missing.
- **Tests**: 3/3 tests passed.

## Step 43: Auto Prioritization Layer (Pre-pick Engine)

We implemented an "Auto Prioritization" layer to minimize human picking effort. The engine now automatically scores and ranks the top 3â€“5 most critical topics, allowing operators to focus on approval rather than complex discovery.

### Key Deliverables
- **Prioritization Logic**: `src/ops/auto_prioritizer.py` applies deterministic, weight-based scoring (+3 for time-sensitivity, +2 for level 3, etc.) to all ready and nearly-ready topics.
- **Top Section UI**: A new dashboard block, `ğŸ”¥ AUTO-PICK PRIORITY (Top 3â€“5)`, surfaces the highest-value candidates with color-coded criticalness badges (ğŸ”´ CRITICAL, ğŸŸ  HIGH, etc.).
- **Smart Fill**: If the "READY" pool is insufficient, the engine automatically draws from `NEARLY_READY` and `READY_TO_PROMOTE` shadow buckets to maintain a healthy recommendation volume.
- **Pipeline Integration**: The prioritizer is wired into the core engine pipeline and manifest, generating `data/ops/auto_priority_today.json` during every run.

### Verification Evidence
- **Scoring Accuracy**: Verified that weighted factors (impact, level, fact-driven) correctly sum and penalties (saturation, ceilings) are applied via `tests/test_auto_prioritizer.py`.
- **Tie-Breaking**: Confirmed stable alphabetical title sorting when topic scores are identical.
- **UI Logic**: Dashboard correctly renders the prioritization panel only when sufficient candidates are available, otherwise showing a data-insufficiency hint.
- **Tests**: 4/4 tests passed.

### [WORK CONFIRMATION]
Step 43-1 â€” Priority weights defined
Step 43-2 â€” Candidate pool applied
Step 43-3 â€” JSON output generated
Step 43-4 â€” Dashboard section rendered
Step 43-5 â€” OPS wiring added
Step 43-6 â€” Tests passing
Push â€” DONE (main)
